#!/usr/bin/env python3
"""
合并之前的两个脚本功能：
- collect: 递归收集 `QCP_examples` 下以 `.c` 和 `.strategies` 结尾的文件，生成
  `examples_and_strategies.json` 与 `examples_lists.py`。
- analyze: 对收集到的文件进行分析，生成 `examples_strategies_stats.json` 与
  `examples_strategies_stats.py`。

在仓库根运行此脚本。
"""
import os
import json
import re
import runpy
import sys
import subprocess
import time
from collections import OrderedDict
import shlex
import datetime
import csv
import shutil
 
BASE_DIR = 'QCP_examples'
OUT_LISTS_JSON = 'examples_and_strategies.json'
OUT_LISTS_PY = 'examples_lists.py'
OUT_STATS_JSON = 'examples_strategies_stats.json'
OUT_STATS_PY = 'examples_strategies_stats.py'
OUT_STATS_CSV = 'examples_strategies_stats.csv'


def collect_lists(base=BASE_DIR):
    examples = []
    strategies = []
    for root, dirs, files in os.walk(base):
        for fn in files:
            rel = os.path.relpath(os.path.join(root, fn), base)
            if fn.endswith('.c'):
                examples.append(rel)
            elif fn.endswith('.strategies'):
                strategies.append(rel)
    examples.sort()
    strategies.sort()
    # write outputs
    lists = {'examples': examples, 'strategies': strategies}
    with open(OUT_LISTS_JSON, 'w', encoding='utf-8') as f:
        json.dump(lists, f, indent=2, ensure_ascii=False)
    with open(OUT_LISTS_PY, 'w', encoding='utf-8') as f:
        f.write('# Auto-generated by scripts/collect_and_analyze.py\n')
        f.write('examples = ' + repr(examples) + '\n')
        f.write('strategies = ' + repr(strategies) + '\n')
    return examples, strategies



# --- analysis helpers (adapted from previous script) ---
id_line_re = re.compile(r'^\s*id\s*:\s*(\d+)', re.IGNORECASE)
# 'struct' was included here to avoid matching struct/enum declarations,
# but it also prevents detecting functions whose return type starts with 'struct'.
# Remove 'struct' to correctly detect functions like 'struct list *foo(...)'.
# allow optional leading closing braces or parens before control keywords (e.g. '} else if (...)')
control_kw_re = re.compile(r'^[\s\)\}]*\b(?:if|for|while|switch|else|typedef|enum)\b')
func_name_re = re.compile(r'[A-Za-z_][A-Za-z0-9_]*\s*\(')


def analyze_c_file(base, path):
    full = os.path.join(base, path)
    res = {'path': path, 'functions': 0, 'comment_lines': 0}
    if not os.path.isfile(full):
        res['error'] = 'not found'
        return res

    with open(full, 'r', encoding='utf-8', errors='ignore') as f:
        raw_lines = f.readlines()
    in_block = False
    block_excluded = False
    # track plain C comment blocks (/* ... */) that are NOT annotation blocks (/*@ ... */)
    in_plain_block = False
    code_lines = []

    for line in raw_lines:
        s = line.rstrip('\n')
        # helper: decide whether a comment-content fragment is meaningful
        def _meaningful_comment_fragment(fr):
            # remove leading whitespace and leading '*' chars often used in block comments
            fr2 = re.sub(r'^[\s\*]+', '', fr)
            return bool(fr2.strip())
        # First handle plain C /* ... */ comment blocks (not '/*@' annotation blocks)
        if in_plain_block:
            if '*/' in s:
                # end of plain comment block, keep remainder after '*/' for further processing
                s = s.split('*/', 1)[1]
                in_plain_block = False
            else:
                # still inside plain comment -> skip entire line
                continue

        # remove any inline plain /* ... */ segments on this line (but preserve '/*@' blocks)
        while True:
            idx = s.find('/*')
            if idx == -1:
                break
            # if it's an annotation block /*@, leave it for the annotation handler
            if idx + 3 <= len(s) and s[idx:idx+3] == '/*@':
                break
            # find end
            endidx = s.find('*/', idx+2)
            if endidx != -1:
                # remove the /* ... */ portion
                s = s[:idx] + s[endidx+2:]
                continue
            else:
                # start of a plain block that continues on next lines
                s = s[:idx]
                in_plain_block = True
                break

        # remove '//' comments
        if '//' in s:
            s = s.split('//', 1)[0]

        # We only count special annotation blocks that start with '/*@'.
        if in_block:
            # we're inside a /*@ ... */ block
            if '*/' in s:
                # handle closing line
                if not block_excluded:
                    # always count the closing line containing '*/'
                    res['comment_lines'] += 1
                after = s.split('*/', 1)[1]
                in_block = False
                block_excluded = False
                if after.strip():
                    code_lines.append(after)
            else:
                if not block_excluded:
                    # count only if the whole line (inside /*@ */) has meaningful content
                    if _meaningful_comment_fragment(s):
                        res['comment_lines'] += 1
            continue

        # not in block: check for opening of /*@
        if '/*@' in s:
            before, _, after = s.partition('/*@')
            after_stripped = after.lstrip()
            # determine whether this annotation block should be excluded
            lower = after_stripped.lower()
            excluded = lower.startswith('extern coq') or lower.startswith('import coq') or lower.startswith('include strategies')
            if '*/' in after:
                # block ends on same line
                if not excluded:
                    # count the single-line annotation block (count opening/closing line)
                    res['comment_lines'] += 1
                after_after = after.split('*/', 1)[1]
                combined = (before + ' ' + after_after).strip()
                if combined:
                    code_lines.append(combined)
            else:
                # multi-line block
                block_excluded = excluded
                in_block = True
                if before.strip():
                    code_lines.append(before)
                if not block_excluded:
                    # count the opening line that contains '/*@' regardless of inner emptiness
                    res['comment_lines'] += 1
            continue

        # otherwise keep the line as code (we do not count '//' or other comments)
        code_lines.append(s)

    # code lines: non-empty lines after removing annotation blocks (approx. LOC)
    code_line_count = 0
    for L in code_lines:
        if L.strip() == '':
            continue
        # treat lines that are purely comment markers as non-code (we already removed /*@ blocks)
        code_line_count += 1
    res['code_lines'] = code_line_count

    # function detection heuristic -- count only definitions, ignore declarations
    funcs = 0
    n = len(code_lines)
    i = 0
    while i < n:
        line = code_lines[i]
        # quick prefilter: must look like a function signature (name followed by '(')
        if ')' in line and func_name_re.search(line):
            if control_kw_re.match(line):
                i += 1
                continue

            # scan forward to decide whether this signature is followed by '{' (definition)
            # or by a ';' (declaration). Skip empty lines and lines that are only a semicolon.
            has_open = '{' in line
            found_decl = False
            j = i + 1
            # if current line contains a semicolon after the signature, treat as declaration
            if ';' in line and line.rfind(')') < line.rfind(';'):
                found_decl = True

            while not has_open and not found_decl and j < n:
                s = code_lines[j].strip()
                if s == '':
                    j += 1
                    continue
                # a line that is just a semicolon means a separated declaration terminator
                if s == ';' or s.endswith(';'):
                    found_decl = True
                    break
                if '{' in s or s.startswith('{'):
                    has_open = True
                    break
                # otherwise treat as some other code (e.g. attributes) and stop scanning
                break

            if has_open and not found_decl:
                funcs += 1
                i = j + 1
                continue
        i += 1

    res['functions'] = funcs
    return res


def analyze_strategies_file(base, path):
    full = os.path.join(base, path)
    res = {'path': path, 'strategy_count': 0}
    if not os.path.isfile(full):
        res['error'] = 'not found'
        return res
    with open(full, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            if id_line_re.match(line):
                res['strategy_count'] += 1
    if res['strategy_count'] == 0:
        with open(full, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            ids = re.findall(r'^\s*id\s*:', content, re.IGNORECASE | re.MULTILINE)
            if ids:
                res['strategy_count'] = len(ids)
    return res


def count_proof_goals_for_example(rel_c_path):
    """Return tuple (auto_count, manual_count, auto_path, manual_path, auto_exists, manual_exists)
    rel_c_path is like 'fme/fme.c' or 'chars.c'."""
    base_no_ext = rel_c_path[:-2] if rel_c_path.lower().endswith('.c') else rel_c_path
    auto_rel = os.path.join('SeparationLogic', 'examples', base_no_ext + '_proof_auto.v')
    manual_rel = os.path.join('SeparationLogic', 'examples', base_no_ext + '_proof_manual.v')
    auto_full = auto_rel
    manual_full = manual_rel
    auto_count = 0
    manual_count = 0
    auto_exists = os.path.isfile(auto_full)
    manual_exists = os.path.isfile(manual_full)
    lemma_re = re.compile(r'^\s*(?:Local\s+)?Lemma\s+proof_of_', re.IGNORECASE)
    if auto_exists:
        with open(auto_full, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if lemma_re.match(line):
                    auto_count += 1
    if manual_exists:
        with open(manual_full, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if lemma_re.match(line):
                    manual_count += 1
    return auto_count, manual_count, auto_rel, manual_rel, auto_exists, manual_exists


def count_manual_proof_stats(manual_rel):
    """Return (per_proof_counts:list, admitted_count:int) for a manual proof file.
    For each Lemma starting with proof_of_ count non-empty lines between the following Proof. and terminating Qed.;
    if terminated by Admitted. treat that proof as 0 and count it toward admitted_count.
    """
    per_proof = []
    admitted_count = 0
    if not os.path.isfile(manual_rel):
        return per_proof, admitted_count
    lemma_re = re.compile(r'^\s*(?:Local\s+)?Lemma\s+(proof_of_[A-Za-z0-9_]+)')
    proof_start_re = re.compile(r'\bProof\.')
    proof_end_re = re.compile(r'^\s*(Qed|Admitted)\.')
    with open(manual_rel, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()

    i = 0
    n = len(lines)
    while i < n:
        m = lemma_re.match(lines[i])
        if m:
            # find Proof.
            j = i + 1
            while j < n and not proof_start_re.search(lines[j]):
                j += 1
            if j >= n:
                # no Proof. found -> count as 0
                per_proof.append(0)
                i += 1
                continue
            # start counting after the Proof. line
            k = j + 1
            count = 0
            admitted = False
            while k < n:
                endm = proof_end_re.match(lines[k])
                if endm:
                    if endm.group(1) == 'Admitted':
                        admitted = True
                    break
                if lines[k].strip() != '':
                    count += 1
                k += 1
            if admitted:
                per_proof.append(0)
                admitted_count += 1
            else:
                per_proof.append(count)
            i = k + 1
        else:
            i += 1
    return per_proof, admitted_count


def get_category_for(path, base, categories_map=None):
    # Priority: explicit categories.json mapping -> header comment -> directory name -> 'uncategorized'
    if categories_map:
        val = categories_map.get(path)
        if val and str(val).strip():
            return str(val).strip()

    # try to read from header comment
    full = os.path.join(base, path)
    if os.path.isfile(full):
        with open(full, 'r', encoding='utf-8', errors='ignore') as f:
            for _ in range(8):
                line = f.readline()
                if not line:
                    break
                m = re.match(r"\s*(?:\/\/|\/\*)\s*Category\s*:\s*(\S+)", line, re.I)
                if m:
                    return m.group(1)

    # fallback to top-level directory name
    parts = path.split(os.sep)
    if len(parts) > 1:
        return parts[0]
    return 'uncategorized'


def parse_run_example_script(script_path='run-example-linux.sh'):
    """Parse run-example-linux.sh and return a mapping input_file -> command_list
    Only looks for lines containing '--input-file' and uses shlex.split to parse the command.
    """
    mapping = {}
    if not os.path.isfile(script_path):
        return mapping
    with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    # split by ';' to get logical commands (script uses ; to separate commands)
    cmds = [c.strip() for c in content.split(';') if c.strip()]
    for cmd in cmds:
        if '--input-file' not in cmd:
            continue
        # normalize whitespace and newlines then shlex split
        norm = ' '.join(cmd.split())
        parts = shlex.split(norm)
        input_val = None
        for i, tok in enumerate(parts):
            if tok.startswith('--input-file='):
                input_val = tok.split('=', 1)[1]
                break
            if tok == '--input-file' and i + 1 < len(parts):
                input_val = parts[i + 1]
                break
        if input_val:
            mapping[input_val] = parts
    return mapping


def run_separationlogic_make_timing(sep_dir='SeparationLogic', logs_dir='logs'):
    """Run `make depend` then `TIMING=1 make` in `sep_dir`.
    Save outputs to logs_dir/sepmake.out and sepmake.err and return a mapping
    from manual proof relative path (like 'SeparationLogic/examples/..._proof_manual.v')
    to timing in seconds (float). If no timing found for a file, it will be absent.
    """
    res = {}
    outp = os.path.join(logs_dir, 'sepmake.out')
    errp = os.path.join(logs_dir, 'sepmake.err')
    os.makedirs(logs_dir, exist_ok=True)

    # run make depend
    p1 = subprocess.run(['make', 'depend'], cwd=sep_dir, capture_output=True, text=True)
    with open(outp, 'ab') as fo, open(errp, 'ab') as fe:
        fo.write(p1.stdout.encode('utf-8', 'ignore'))
        fe.write(p1.stderr.encode('utf-8', 'ignore'))

    # run TIMING=1 make
    env = os.environ.copy()
    env['TIMING'] = '1'
    p2 = subprocess.run(['make'], cwd=sep_dir, capture_output=True, text=True, env=env)
    with open(outp, 'ab') as fo, open(errp, 'ab') as fe:
        fo.write(p2.stdout.encode('utf-8', 'ignore'))
        fe.write(p2.stderr.encode('utf-8', 'ignore'))
    combined = (p2.stdout or '') + '\n' + (p2.stderr or '')

    # parse combined output for occurrences of *_proof_manual.v and nearby time info
    content = combined
    # regex to find path occurrences
    path_re = re.compile(r'(?P<path>\S*_proof_manual\.v)')
    time_s_re = re.compile(r'(?P<secs>\d+\.\d+)\s*s', re.I)
    time_min_s_re = re.compile(r'(?P<mins>\d+)m(?P<secs>\d+\.\d+)s', re.I)
    time_ms_re = re.compile(r'(?P<msecs>\d+)\s*ms', re.I)

    for m in path_re.finditer(content):
        p = m.group('path')
        # search forward small window for time
        start = m.end()
        window = content[start:start+300]
        t = None
        mm = time_s_re.search(window)
        if mm:
            t = float(mm.group('secs'))
        else:
            mm2 = time_min_s_re.search(window)
            if mm2:
                t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
            else:
                mm3 = time_ms_re.search(window)
                if mm3:
                    t = float(mm3.group('msecs')) / 1000.0

        # if not found forward, try backward window
        if t is None:
            back_start = max(0, m.start()-300)
            windowb = content[back_start:m.start()]
            mm = time_s_re.search(windowb)
            if mm:
                t = float(mm.group('secs'))
            else:
                mm2 = time_min_s_re.search(windowb)
                if mm2:
                    t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
                else:
                    mm3 = time_ms_re.search(windowb)
                    if mm3:
                        t = float(mm3.group('msecs')) / 1000.0

        # normalize path to repository-root relative (prefix SeparationLogic if needed)
        norm = p.replace('\\', '/')
        if not norm.startswith('SeparationLogic/'):
            norm = os.path.join('SeparationLogic', norm).replace('\\', '/')
        if t is not None:
            res[norm] = t

    return res, outp, errp


def parse_sepmake_content(content):
    """Parse sepmake combined stdout+stderr content and return mapping of manual path -> seconds.
    Supports ms, s, and m:ss formats. Keys are normalized to 'SeparationLogic/...' paths.
    """
    res = {}
    path_re = re.compile(r'(?P<path>\S*_proof_manual\.v)')
    time_s_re = re.compile(r'(?P<secs>\d+\.\d+)\s*s', re.I)
    time_min_s_re = re.compile(r'(?P<mins>\d+)m(?P<secs>\d+\.\d+)s', re.I)
    time_ms_re = re.compile(r'(?P<msecs>\d+)\s*ms', re.I)

    for m in path_re.finditer(content):
        p = m.group('path')
        start = m.end()
        window = content[start:start+400]
        t = None
        mm = time_s_re.search(window)
        if mm:
            t = float(mm.group('secs'))
        else:
            mm2 = time_min_s_re.search(window)
            if mm2:
                t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
            else:
                mm3 = time_ms_re.search(window)
                if mm3:
                    t = float(mm3.group('msecs')) / 1000.0

        if t is None:
            back_start = max(0, m.start()-400)
            windowb = content[back_start:m.start()]
            mm = time_s_re.search(windowb)
            if mm:
                t = float(mm.group('secs'))
            else:
                mm2 = time_min_s_re.search(windowb)
                if mm2:
                    t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
                else:
                    mm3 = time_ms_re.search(windowb)
                    if mm3:
                        t = float(mm3.group('msecs')) / 1000.0

        norm = p.replace('\\', '/')
        if not norm.startswith('SeparationLogic/'):
            norm = os.path.join('SeparationLogic', norm).replace('\\', '/')
        if t is not None:
            res[norm] = t * 1000

    return res


def parse_time_from_log_file(logpath, manual_rel):
    """Read a log file and parse timing for manual_rel. Returns seconds or None."""
    if not os.path.isfile(logpath):
        return None
    with open(logpath, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    # try exact match first
    res = parse_sepmake_content(content)
    key = manual_rel.replace('\\', '/')
    if key in res:
        return res[key]
    # fallback to basename match
    base = os.path.basename(manual_rel)
    for k, v in res.items():
        if k.endswith('/' + base):
            return v
    return None


def run_make_and_get_time_for_manual(manual_rel, sep_dir='SeparationLogic', logs_dir='logs', label=''):
    """Run `TIMING=1 make` in sep_dir, save logs to logs_dir/<label>.sepmake.out/.err,
    and parse timing for given manual_rel path. Returns time in seconds or None.
    """
    outp = os.path.join(logs_dir, f'{label}.sepmake.out')
    errp = os.path.join(logs_dir, f'{label}.sepmake.err')
    env = os.environ.copy()
    env['TIMING'] = '1'
    p = subprocess.run(['make'], cwd=sep_dir, capture_output=True, text=True, env=env)
    with open(outp, 'wb') as fo:
        fo.write((p.stdout or '').encode('utf-8', 'ignore'))
    with open(errp, 'wb') as fe:
        fe.write((p.stderr or '').encode('utf-8', 'ignore'))
    combined = (p.stdout or '') + '\n' + (p.stderr or '')

    # parse for the specific manual_rel (allow matching by suffix)
    # support ms, s, mm:ss formats already available
    time_ms_re = re.compile(r'(?P<msecs>\d+)\s*ms', re.I)
    time_s_re = re.compile(r'(?P<secs>\d+\.\d+)\s*s', re.I)
    time_min_s_re = re.compile(r'(?P<mins>\d+)m(?P<secs>\d+\.\d+)s', re.I)

    # find occurrences of the manual filename (without leading path) and search nearby for time
    short = os.path.basename(manual_rel)
    idx = combined.find(short)
    if idx == -1:
        # try full relative path
        idx = combined.find(manual_rel.replace('\\', '/'))
    if idx == -1:
        return None

    # search forward/backward windows
    start = idx + len(short)
    window = combined[start:start+400]
    t = None
    mm = time_s_re.search(window)
    if mm:
        t = float(mm.group('secs'))
    else:
        mm2 = time_min_s_re.search(window)
        if mm2:
            t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
        else:
            mm3 = time_ms_re.search(window)
            if mm3:
                t = float(mm3.group('msecs')) / 1000.0

    if t is None:
        back_start = max(0, idx-400)
        windowb = combined[back_start:idx]
        mm = time_s_re.search(windowb)
        if mm:
            t = float(mm.group('secs'))
        else:
            mm2 = time_min_s_re.search(windowb)
            if mm2:
                t = int(mm2.group('mins')) * 60.0 + float(mm2.group('secs'))
            else:
                mm3 = time_ms_re.search(windowb)
                if mm3:
                    t = float(mm3.group('msecs')) / 1000.0

    return t


def run_gen_backup_for_example(full_cmd, logs_dir='logs', label='gen'):
    """Run the provided command plus --gen-and-backup to produce manual backup files.
    Writes logs to logs_dir/<label>.gen.out/.err. Returns True on success (exit code 0), False otherwise.
    """
    if not full_cmd:
        return False
    cmd = list(full_cmd) + ['--gen-and-backup']
    outp = os.path.join(logs_dir, f'{label}.gen.out')
    errp = os.path.join(logs_dir, f'{label}.gen.err')
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    with open(outp, 'wb') as fo:
        fo.write(p.stdout)
    with open(errp, 'wb') as fe:
        fe.write(p.stderr)
    return p.returncode == 0


def analyze_lists(base, examples, strategies, categories_map=None):
    # try to parse run-example-linux.sh to get full command templates per input file
    run_example_map = parse_run_example_script('run-example-linux.sh')
    # allow using an existing logs directory instead of running make
    use_existing = os.environ.get('USE_EXISTING_LOG_DIR')
    if use_existing and os.path.isdir(use_existing):
        run_logs = use_existing
        skip_make = True
    else:
        # create a timestamped logs directory for this run: logs/log-YYYY-MM-DD-HH-MM-SS
        now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        run_logs = os.path.join('logs', 'log-' + now)
        os.makedirs(run_logs, exist_ok=True)
        skip_make = False
    examples_stats = []
    for p in examples:
        stats = analyze_c_file(base, p)
        stats['category'] = get_category_for(p, base, categories_map)
        # count proof goals in SeparationLogic examples
        auto_cnt, manual_cnt, auto_rel, manual_rel, auto_exists, manual_exists = count_proof_goals_for_example(p)
        stats['proof_goals_auto'] = auto_cnt
        stats['proof_goals_manual'] = manual_cnt
        stats['proof_auto_path'] = auto_rel
        stats['proof_manual_path'] = manual_rel
        stats['proof_auto_exists'] = auto_exists
        stats['proof_manual_exists'] = manual_exists
        # per-proof manual proof code line counts and Admitted detection
        per_proof_lines, admitted_cnt = count_manual_proof_stats(manual_rel) if manual_exists else ([], 0)
        stats['proof_manual_proof_lines_total'] = sum(per_proof_lines)
        stats['proof_manual_has_admitted'] = (admitted_cnt > 0)
        stats['proof_manual_admitted_count'] = admitted_cnt
        # symbolic execution time for this example
        inp = os.path.join('QCP_examples', p)
        full_cmd = None
        if inp in run_example_map:
            full_cmd = run_example_map[inp]
        sym_time = run_and_time_input(inp, is_strategy=False, full_cmd=full_cmd, log_prefix=p.replace('/', '_'), logs_dir=run_logs)
        stats['symbolic_time'] = sym_time if sym_time is not None else -1
        examples_stats.append(stats)

    for s in examples_stats :
      if s['symbolic_time'] == -1:
        # if symbolic time is not available, set it to None
        sys.exit(f"Error: symbolic execution fail. Check logs in {run_logs}.")
  

    # For each example with manual proof, perform the revised timing algorithm:
    # 1) run generator with --gen-and-backup to produce <manual>_backup.v
    # 2) run `TIMING=1 make` once and parse time for manual file -> t1
    # 3) copy backup content over manual, run make again -> t2
    # 4) final time = t2 - t1; restore original manual and remove backup
    for e in examples_stats:
        manual_rel = e.get('proof_manual_path')
        e['make_manual_time'] = -1
        if not manual_rel or not e.get('proof_manual_exists'):
            continue

        # label for logs
        name_label = e['path'].replace('/', '_')

        # attempt to run generator to create backup if we have full command
        inp = os.path.join('QCP_examples', e['path'])
        full_cmd = run_example_map.get(inp)
        if not skip_make:
            # attempt to run generator to create backup if we have full command
            if full_cmd:
                run_gen_backup_for_example(full_cmd, logs_dir=run_logs, label=name_label + '_gen')

            # first make to get t1
            t1 = run_make_and_get_time_for_manual(manual_rel, sep_dir=os.path.join('.', 'SeparationLogic'), logs_dir=run_logs, label=name_label + '_make1')

            # locate backup file
            backup_rel = manual_rel.replace('_proof_manual.v', '_proof_manual_backup1.v')
            backup_exists = os.path.isfile(backup_rel)
            if not backup_exists:
                # try alternative suffix if generator uses different naming
                alt = manual_rel.replace('_proof_manual.v', '_manual_backup1.v')
                if os.path.isfile(alt):
                    backup_rel = alt
                    backup_exists = True

            if not backup_exists:
                # cannot proceed to second make; record baseline if present
                e['make_manual_time'] = t1 if t1 is not None else -1
                continue

            # copy backup content over manual
            shutil.copyfile(backup_rel, manual_rel)

            # second make to get t2
            t2 = run_make_and_get_time_for_manual(manual_rel, sep_dir=os.path.join('.', 'SeparationLogic'), logs_dir=run_logs, label=name_label + '_make2')
      
            # remove backup file
            if os.path.isfile(backup_rel):
                os.remove(backup_rel)
        else:
            # skip actual make: parse existing logs for make1/make2
            t1 = parse_time_from_log_file(os.path.join(run_logs, name_label + '_make1.sepmake.out'), manual_rel)
            if t1 is None:
                t1 = parse_time_from_log_file(os.path.join(run_logs, name_label + '_make1.sepmake.err'), manual_rel)
            t2 = parse_time_from_log_file(os.path.join(run_logs, name_label + '_make2.sepmake.out'), manual_rel)
            if t2 is None:
                t2 = parse_time_from_log_file(os.path.join(run_logs, name_label + '_make2.sepmake.err'), manual_rel)

        if t1 is None or t2 is None:
            e['make_manual_time'] = -1
        else:
            # final time is the delta
            delta = float(t2) - float(t1)
            e['make_manual_time'] = delta if delta >= 0 else 0

    strategies_stats = [analyze_strategies_file(base, p) for p in strategies]
    # measure symbolic time for each strategy file
    for s in strategies_stats:
        inp = os.path.join('QCP_examples', s['path'])
        full_cmd = None
        if inp in run_example_map:
            full_cmd = run_example_map[inp]
        t = run_and_time_input(inp, is_strategy=True, full_cmd=full_cmd, log_prefix=s['path'].replace('/', '_'), logs_dir=run_logs)
        s['symbolic_time'] = t if t is not None else -1

    # determine global sepmake log paths if present
    make_out_path = os.path.join(run_logs, 'sepmake.out') if os.path.isfile(os.path.join(run_logs, 'sepmake.out')) else None
    make_err_path = os.path.join(run_logs, 'sepmake.err') if os.path.isfile(os.path.join(run_logs, 'sepmake.err')) else None

    out = {'examples': examples_stats, 'strategies': strategies_stats, 'make_timing_out': make_out_path, 'make_timing_err': make_err_path}
    with open(OUT_STATS_JSON, 'w', encoding='utf-8') as f:
        json.dump(out, f, indent=2, ensure_ascii=False)
    with open(OUT_STATS_PY, 'w', encoding='utf-8') as f:
        f.write('# Auto-generated by scripts/collect_and_analyze.py\\n')
        f.write('examples_stats = ' + repr(examples_stats) + '\\n')
        f.write('strategies_stats = ' + repr(strategies_stats) + '\\n')
    # write a CSV summary for examples (name = file base without extension), omit proof path fields
    with open(OUT_STATS_CSV, 'w', encoding='utf-8', newline='') as cf:
        writer = csv.writer(cf)
        # header (note: 'statement_lines' removed; use 'code_lines')
        header = ['name','path','category','functions','code_lines','comment_lines','proof_goals_auto','proof_goals_manual','proof_manual_proof_lines_total','proof_manual_has_admitted','proof_manual_admitted_count','symbolic_time','make_manual_time']
        writer.writerow(header)
        for e in examples_stats:
            name = os.path.splitext(os.path.basename(e.get('path','')))[0]
            row = [
                name,
                e.get('path',''),
                e.get('category',''),
                e.get('functions',0),
                e.get('code_lines',0),
                e.get('comment_lines',0),
                e.get('proof_goals_auto',0),
                e.get('proof_goals_manual',0),
                e.get('proof_manual_proof_lines_total',0),
                e.get('proof_manual_has_admitted',False),
                e.get('proof_manual_admitted_count',0),
                e.get('symbolic_time',-1),
                e.get('make_manual_time',-1)
            ]
            writer.writerow(row)
    # report files that failed symbolic execution (symbolic_time == -1)
    fail_examples = [e for e in examples_stats if e.get('symbolic_time', -1) == -1]
    fail_strats = [s for s in strategies_stats if s.get('symbolic_time', -1) == -1]
    fail_count = len(fail_examples) + len(fail_strats)
    failures = []
    if fail_count > 0:
        for e in fail_examples:
            name = e['path'].replace('/', '_')
            outp = os.path.join(run_logs, name + '.out')
            errp = os.path.join(run_logs, name + '.err')
            out_sz = os.path.getsize(outp) if os.path.isfile(outp) else 0
            err_sz = os.path.getsize(errp) if os.path.isfile(errp) else 0
            failures.append({'path': e['path'], 'type': 'example', 'out': outp, 'err': errp, 'out_bytes': out_sz, 'err_bytes': err_sz})
        for s in fail_strats:
            name = s['path'].replace('/', '_')
            outp = os.path.join(run_logs, name + '.out')
            errp = os.path.join(run_logs, name + '.err')
            out_sz = os.path.getsize(outp) if os.path.isfile(outp) else 0
            err_sz = os.path.getsize(errp) if os.path.isfile(errp) else 0
            failures.append({'path': s['path'], 'type': 'strategy', 'out': outp, 'err': errp, 'out_bytes': out_sz, 'err_bytes': err_sz})
        # write failures.json in run_logs
        with open(os.path.join(run_logs, 'failures.json'), 'w', encoding='utf-8') as ff:
            json.dump(failures, ff, indent=2, ensure_ascii=False)
        print(f"{fail_count} files symbolic execution error, the log are put in {run_logs}", file=sys.stderr)
    return out


def run_and_time_input(input_rel, is_strategy=False, timeout=None, full_cmd=None, log_prefix=None, logs_dir=None):
    """Run the corresponding binary with --input-file and measure wall-clock time.
    Returns elapsed seconds (float) or -1 if failed or no time.
    """
    # By default do not enforce a timeout unless SYMB_EXEC_TIMEOUT is explicitly set
    if timeout is None:
        env_val = os.environ.get('SYMB_EXEC_TIMEOUT')
        if env_val is not None and re.match(r'^\s*-?\d+\s*$', str(env_val)):
            timeout = int(env_val.strip())

    # If full_cmd is provided by the caller use it, otherwise build a default command
    if full_cmd:
        cmd = list(full_cmd)
    else:
        binary = 'linux-binary/StrategyCheck' if is_strategy else 'linux-binary/symexec'
        cmd = [binary, f'--input-file={input_rel}', '--no-exec-info']

    if logs_dir is None:
        logs_dir = 'logs'
    os.makedirs(logs_dir, exist_ok=True)

    name = log_prefix or os.path.basename(input_rel).replace('/', '_')
    out_path = os.path.join(logs_dir, name + '.out')
    err_path = os.path.join(logs_dir, name + '.err')

    # Resolve command path to avoid FileNotFoundError without try/except
    if not cmd or not cmd[0]:
        with open(err_path, 'w', encoding='utf-8') as fe:
            fe.write('executable not found: ' + (cmd[0] if cmd and len(cmd) > 0 else str(cmd)))
        return -1

    resolved = cmd[0]
    if not os.path.isabs(resolved):
        located = shutil.which(resolved)
        if located:
            cmd = [located] + list(cmd[1:])
            resolved = cmd[0]
    elif not os.path.isfile(resolved):
        located = shutil.which(resolved)
        if located:
            cmd = [located] + list(cmd[1:])
            resolved = cmd[0]

    abs_missing = os.path.isabs(resolved) and not os.path.isfile(resolved)
    rel_missing = (not os.path.isabs(resolved)) and shutil.which(resolved) is None and not os.path.isfile(resolved)
    if abs_missing or rel_missing:
        with open(err_path, 'w', encoding='utf-8') as fe:
            fe.write('executable not found: ' + resolved)
        return -1

    start = time.time()
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    stdout = b''
    stderr = b''
    if timeout is None or timeout <= 0:
        stdout, stderr = proc.communicate()
    else:
        deadline = start + timeout
        while proc.poll() is None and time.time() < deadline:
            time.sleep(0.05)
        if proc.poll() is None:
            proc.kill()
            stdout, stderr = proc.communicate()
            with open(out_path, 'wb') as fo:
                fo.write(stdout)
            with open(err_path, 'wb') as fe:
                fe.write(stderr or b'timeout')
            return -1
        stdout, stderr = proc.communicate()

    end = time.time()
    has_err = bool(stderr and stderr.strip())
    if has_err:
        with open(out_path, 'wb') as fo:
            fo.write(stdout)
        with open(err_path, 'wb') as fe:
            fe.write(stderr)
        return -1

    return (end - start) * 1000.0


def main():
    # support a quick mode to only recompute code_lines for all examples and exit
    if '--recompute-code-lines' in sys.argv:
        examples, strategies = collect_lists()
        outmap = {}
        for p in examples:
            stats = analyze_c_file(BASE_DIR, p)
            outmap[p] = {
                'code_lines': stats.get('code_lines', 0),
                'functions': stats.get('functions', 0),
                'comment_lines': stats.get('comment_lines', 0)
            }
        with open('examples_code_lines.json', 'w', encoding='utf-8') as fo:
            json.dump(outmap, fo, indent=2, ensure_ascii=False)
        # also write CSV for easy consumption (include comment_lines)
        with open('examples_code_lines.csv', 'w', encoding='utf-8', newline='') as cf:
            writer = csv.writer(cf)
            writer.writerow(['path', 'code_lines', 'comment_lines', 'functions'])
            for k in sorted(outmap.keys()):
                v = outmap[k]
                writer.writerow([k, v.get('code_lines', 0), v.get('comment_lines', 0), v.get('functions', 0)])
        print(f'Wrote examples_code_lines.json and examples_code_lines.csv for {len(outmap)} files')
        return

    examples, strategies = collect_lists()
    print(f'Collected {len(examples)} .c files and {len(strategies)} .strategies files')
    # Load categories.json if present
    categories_map = None
    if os.path.isfile('categories.json'):
        with open('categories.json', 'r', encoding='utf-8') as f:
            categories_map = json.load(f)

    # If categories mapping exists, ensure all examples are annotated (non-empty)
    if categories_map:
        uncategorized = [p for p in examples if p not in categories_map or str(categories_map.get(p, '')).strip() == '']
        if uncategorized:
            # Append uncategorized entries to categories.json (preserve original order, add new keys at end)
            merged = OrderedDict()
            for k, v in categories_map.items():
                merged[k] = v
            for k in uncategorized:
                merged[k] = ""
            with open('categories.json', 'w', encoding='utf-8') as f:
                json.dump(merged, f, indent=2, ensure_ascii=False)

            # Report and exit with error
            print('Find uncategorized files', file=sys.stderr)
            for p in uncategorized:
                print(' -', p, file=sys.stderr)
            sys.exit(2)

    stats = analyze_lists(BASE_DIR, examples, strategies, categories_map)
    print(f'Analyzed {len(stats["examples"])} .c files and {len(stats["strategies"])} .strategies files')


if __name__ == '__main__':
    main()
